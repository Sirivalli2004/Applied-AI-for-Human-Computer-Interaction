{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"emotion train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOpKteguFKuNx5J/gDaFPT1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d-WzO8LNnjMs"},"outputs":[],"source":["import keras\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import os\n","\n","num_classes = 6\n","\n","Img_Height = 48\n","Img_width = 48\n","\n","batch_size = 32\n","train_directory = \"drive/MyDrive/demo/train\"\n","train_dir = os.listdir(train_directory)\n","validation_directory = \"drive/MyDrive/demo/validation\"\n","validation_dir = os.listdir(validation_directory)\n","\n","\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=60,\n","                                   shear_range=0.5,\n","                                   zoom_range=0.5,\n","                                   width_shift_range=0.5,\n","                                   height_shift_range=0.5,\n","                                   horizontal_flip=True,\n","                                   fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_directory,\n","                                                    color_mode='grayscale',\n","                                                    target_size=(Img_Height, Img_width),\n","                                                    batch_size=batch_size,\n","                                                    class_mode='categorical',\n","                                                    shuffle=True)\n","\n","validation_generator = validation_datagen.flow_from_directory(validation_directory,\n","                                                              color_mode='grayscale',\n","                                                              target_size=(Img_Height, Img_width),\n","                                                              batch_size=batch_size,\n","                                                              class_mode='categorical',\n","                                                              shuffle=True)\n","\n","\n","model = Sequential()\n","\n","# Block-1: The First Convolutional Block\n","\n","model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n","                 kernel_initializer='he_normal',\n","                 activation=\"elu\", \n","                 input_shape=(Img_Height, Img_width, 1), \n","                 name=\"Conv1\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm1\"))\n","\n","model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', \n","                 kernel_initializer='he_normal', \n","                 activation=\"elu\", name=\"Conv2\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm2\"))\n","model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool1\"))\n","model.add(Dropout(0.2, name=\"Dropout1\"))\n","\n","# Block-2: The Second Convolutional Block\n","\n","model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', \n","                 kernel_initializer='he_normal',\n","                 activation=\"elu\", name=\"Conv3\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm3\"))\n","\n","model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',\n","                 kernel_initializer='he_normal', \n","                 activation=\"elu\", name=\"Conv4\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm4\"))\n","model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool2\"))\n","model.add(Dropout(0.2, name=\"Dropout2\"))\n","\n","# Block-3: The Third Convolutional Block\n","\n","model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n","                 kernel_initializer='he_normal', \n","                 activation=\"elu\", name=\"Conv5\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm5\"))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', \n","                 kernel_initializer='he_normal',\n","                 activation=\"elu\", name=\"Conv6\"))\n","\n","model.add(BatchNormalization(name=\"Batch_Norm6\"))\n","model.add(MaxPooling2D(pool_size=(2,2), name=\"Maxpool3\"))\n","model.add(Dropout(0.2, name=\"Dropout3\"))\n","\n","# Block-4: The Fully Connected Block\n","\n","model.add(Flatten(name=\"Flatten\"))\n","model.add(Dense(64, activation=\"elu\", kernel_initializer='he_normal', name=\"Dense\"))\n","model.add(BatchNormalization(name=\"Batch_Norm7\"))\n","model.add(Dropout(0.5, name=\"Dropout4\"))\n","\n","# Block-5: The Output Block\n","\n","model.add(Dense(num_classes, activation=\"softmax\", kernel_initializer='he_normal', name = \"Output\"))\n","\n","\n","\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","checkpoint = ModelCheckpoint(\"drive/MyDrive/demo/saved_model/demo_emotions.h5\", monitor='accuracy', verbose=1,\n","                              save_best_only=True, mode='auto', period=1)\n","\n","reduce = ReduceLROnPlateau(monitor='accuracy', factor=0.2, patience=10, \n","                           min_lr=0.0001, verbose = 1)\n","\n","logdir='logs'\n","tensorboard_Visualization = TensorBoard(log_dir=logdir, histogram_freq=False)\n","\n","\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer = Adam(lr = 0.001),\n","              metrics=['accuracy'])\n","\n","\n","train_samples = 28353\n","validation_samples = 3534\n","epochs = 150\n","batch_size = 64\n","\n","model.fit(train_generator,\n","          steps_per_epoch = train_samples//batch_size,\n","          epochs = epochs,\n","          callbacks = [checkpoint, reduce, tensorboard_Visualization],\n","          validation_data = validation_generator,\n","          validation_steps = validation_samples//batch_size,\n","          shuffle=True)\n","\n","\n","model.save('drive/MyDrive/demo/saved_model/demo_emotions_model.h5')"]}]}